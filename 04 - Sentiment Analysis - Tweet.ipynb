{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment.ipynb","provenance":[],"authorship_tag":"ABX9TyO8WPoWBEjkGSWODEBBY/zR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTIFQAXdwrL4","executionInfo":{"status":"ok","timestamp":1633207644668,"user_tz":-120,"elapsed":7810,"user":{"displayName":"xi xi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07701880042082875819"}},"outputId":"d93fc1b5-f615-4b83-878a-539bfc646835"},"source":["pip install nltk==3.3"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nltk==3.3\n","  Downloading nltk-3.3.0.zip (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3) (1.15.0)\n","Building wheels for collected packages: nltk\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394487 sha256=3283ebc187052858af074b341e66a8e1dd4516a92a129dad7a23f2e27a2a626b\n","  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n","Successfully built nltk\n","Installing collected packages: nltk\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.3\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99SWc0q2w3w0","executionInfo":{"status":"ok","timestamp":1633207773291,"user_tz":-120,"elapsed":1354,"user":{"displayName":"xi xi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07701880042082875819"}},"outputId":"b89353ac-8799-42fc-a711-f65155cd3811"},"source":["import nltk\n","nltk.download('twitter_samples')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhTmNi5iwdFJ","executionInfo":{"status":"ok","timestamp":1633207800674,"user_tz":-120,"elapsed":16154,"user":{"displayName":"xi xi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07701880042082875819"}},"outputId":"0720dfd0-5f5e-41cf-e61f-62d17a5c416e"},"source":["from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.corpus import twitter_samples, stopwords\n","from nltk.tag import pos_tag\n","from nltk.tokenize import word_tokenize\n","from nltk import FreqDist, classify, NaiveBayesClassifier\n","\n","import re, string, random\n","\n","def remove_noise(tweet_tokens, stop_words = ()):\n","\n","    cleaned_tokens = []\n","\n","    for token, tag in pos_tag(tweet_tokens):\n","        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n","                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n","        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n","\n","        if tag.startswith(\"NN\"):\n","            pos = 'n'\n","        elif tag.startswith('VB'):\n","            pos = 'v'\n","        else:\n","            pos = 'a'\n","\n","        lemmatizer = WordNetLemmatizer()\n","        token = lemmatizer.lemmatize(token, pos)\n","\n","        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n","            cleaned_tokens.append(token.lower())\n","    return cleaned_tokens\n","\n","def get_all_words(cleaned_tokens_list):\n","    for tokens in cleaned_tokens_list:\n","        for token in tokens:\n","            yield token\n","\n","def get_tweets_for_model(cleaned_tokens_list):\n","    for tweet_tokens in cleaned_tokens_list:\n","        yield dict([token, True] for token in tweet_tokens)\n","\n","if __name__ == \"__main__\":\n","\n","    positive_tweets = twitter_samples.strings('positive_tweets.json')\n","    negative_tweets = twitter_samples.strings('negative_tweets.json')\n","    text = twitter_samples.strings('tweets.20150430-223406.json')\n","    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n","\n","    stop_words = stopwords.words('english')\n","\n","    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n","    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n","\n","    positive_cleaned_tokens_list = []\n","    negative_cleaned_tokens_list = []\n","\n","    for tokens in positive_tweet_tokens:\n","        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n","\n","    for tokens in negative_tweet_tokens:\n","        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n","\n","    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n","\n","    freq_dist_pos = FreqDist(all_pos_words)\n","    print(freq_dist_pos.most_common(10))\n","\n","    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n","    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n","\n","    positive_dataset = [(tweet_dict, \"Positive\")\n","                         for tweet_dict in positive_tokens_for_model]\n","\n","    negative_dataset = [(tweet_dict, \"Negative\")\n","                         for tweet_dict in negative_tokens_for_model]\n","\n","    dataset = positive_dataset + negative_dataset\n","\n","    random.shuffle(dataset)\n","\n","    train_data = dataset[:7000]\n","    test_data = dataset[7000:]\n","\n","    classifier = NaiveBayesClassifier.train(train_data)\n","\n","    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n","\n","    print(classifier.show_most_informative_features(10))\n","\n","    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n","\n","    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n","\n","    print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n","Accuracy is: 0.9946666666666667\n","Most Informative Features\n","                      :) = True           Positi : Negati =   1000.5 : 1.0\n","                followed = True           Negati : Positi =     23.7 : 1.0\n","                follower = True           Positi : Negati =     22.5 : 1.0\n","                     bam = True           Positi : Negati =     20.3 : 1.0\n","                 welcome = True           Positi : Negati =     19.7 : 1.0\n","                     sad = True           Negati : Positi =     18.2 : 1.0\n","                     x15 = True           Negati : Positi =     16.4 : 1.0\n","               community = True           Positi : Negati =     16.3 : 1.0\n","               goodnight = True           Positi : Negati =     13.6 : 1.0\n","                    glad = True           Positi : Negati =     13.4 : 1.0\n","None\n","I ordered just once from TerribleCo, they screwed up, never used the app again. Negative\n"]}]}]}